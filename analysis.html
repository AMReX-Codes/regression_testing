
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Analysis Features &#8212; regression_testing  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Configuration Options" href="configuration.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="analysis-features">
<h1>Analysis Features<a class="headerlink" href="#analysis-features" title="Permalink to this heading">¶</a></h1>
<section id="custom-analysis-routines">
<h2>Custom Analysis Routines<a class="headerlink" href="#custom-analysis-routines" title="Permalink to this heading">¶</a></h2>
<p>Each test problem may optionally be equipped with a user-defined analysis
routine, which takes in the output data file and up to one other argument,
and exits with a nonzero return code on a failure. The suite requires the
relative path to the executable from the root directory of the build repository,
specified via the <code class="docutils literal notranslate"><span class="pre">analysisRoutine</span></code> parameter in the configuration file. Two
additional parameters may be supplied - <code class="docutils literal notranslate"><span class="pre">analysisMainArgs</span></code> and
<code class="docutils literal notranslate"><span class="pre">analysisOutputImage</span></code>. The former must be the name of a member variable of the
Suite class (in suite.py), which will be accessed and provided to the routine as
the first argument. The latter is the name of the routine’s output file, which
will be copied to the web directory and linked to from the test problem webpage
if set in the configuration file.</p>
<p>With a routine named <code class="docutils literal notranslate"><span class="pre">analysis.py</span></code>, analysisMainArgs set to <code class="docutils literal notranslate"><span class="pre">source_dir</span></code>,
and a data file named <code class="docutils literal notranslate"><span class="pre">plt0091</span></code>, the routine would be executed from its parent
directory with the following shell command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ ./analysis.py &lt;suite.source_dir&gt; plt0091
</pre></div>
</div>
<p>An example script may be found <a class="reference external" href="https://github.com/AMReX-Astro/Castro/blob/master/Exec/hydro_tests/Sod_stellar/testsuite_analysis/test1-helm.py">here</a>.</p>
</section>
<section id="monitoring-performance">
<h2>Monitoring Performance<a class="headerlink" href="#monitoring-performance" title="Permalink to this heading">¶</a></h2>
<p>The toolkit provides features for tracking performance across multiple runs of
a test suite, and warning the developer upon detecting a drop in efficiency.
Plots showing the performance of each test across its entire run history are
automatically generated and linked to at the head of each column on the suite’s
main index page. Automated performance monitoring is also available, but is
off by default.</p>
<p>To turn on performance monitoring for an individual test problem, add the line
<code class="docutils literal notranslate"><span class="pre">check_performance</span> <span class="pre">=</span> <span class="pre">1</span></code> to the problem’s section in the configuration file.
Following completion of each run of the test problem, its execution time will be
compared with a running average, triggering a warning if the ratio of the two
(execution time / running average) fails to meet a certain threshold. This
threshold may be specified using the <code class="docutils literal notranslate"><span class="pre">performance_threshold</span></code> parameter; if
omitted it will default to 1.2, or a 20% drop in performance. The number of
past runs to include in the average may also be specified with the
<code class="docutils literal notranslate"><span class="pre">runs_to_average</span></code> parameter, which is set to 5 by default.</p>
<p>The same feature may be enabled for the entire suite by supplying the
–check_performance flag on the command line. For a performance threshold of 1.1
and averaging the last 5 runs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ ./regtest.py --check_performance <span class="s2">&quot;1.1&quot;</span> <span class="m">5</span> configuration_file
</pre></div>
</div>
<p>The performance check will then be made for every test problem using those
parameters. Quotes on the parameters are optional.</p>
</section>
<section id="parameter-coverage">
<h2>Parameter Coverage<a class="headerlink" href="#parameter-coverage" title="Permalink to this heading">¶</a></h2>
<p>Parameter coverage reports may also be automatically generated for Castro and
MAESTRO applications, or others that produce job_info files of the same format.
The required format consists of a clearly defined section header containing the
“Parameter” keyword, at least one line of separation (may be anything), and then
a list of parameters starting at the next “=” sign. Covered parameters should be
marked with a [*], and the parameter section should be the last one in the
file.</p>
<p>If this feature is enabled, the suite will output files titled coverage.out
and coverage_nonspecific.out to the suite run directory under &#64;suiteName&#64;-tests.
The former lists all parameters left uncovered by the test suite, including
those specific to individual test problems, and gives counts of the covered
and uncovered parameters along with the overall coverage percentage. The
latter contains the same information, but omitting test-specific parameters.
These two files are also copied to the corresponding web directory, and
linked to from a table on the test run’s index page. The table displays the
coverage percentage and parameter counts from the outfiles.</p>
<p>To enable parameter coverage, add <code class="docutils literal notranslate"><span class="pre">reportCoverage</span> <span class="pre">=</span> <span class="pre">1</span></code> to the [main] section
of the configuration file. The –with_coverage flag may also be supplied to
the executable, having the same effect:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ ./regtest.py --with_coverage configuration_file
</pre></div>
</div>
</section>
<section id="skipping-comparison">
<h2>Skipping Comparison<a class="headerlink" href="#skipping-comparison" title="Permalink to this heading">¶</a></h2>
<p>As these routines were designed primarily for regression testing, a comparison
to pre-generated benchmarks is made for each test problem after execution. In
the event that this behavior is not desired (perhaps all analysis is done via
a Python script, as described in <a class="reference internal" href="#custom-analysis-routines">Custom Analysis Routines</a>), comparison to
benchmarks may be disabled for an individual test problem by adding the line
<code class="docutils literal notranslate"><span class="pre">doComparison</span> <span class="pre">=</span> <span class="pre">0</span></code> to the problem’s section in the configuration file.
Comparison to benchmarks may also be disabled globally by supplying the
–skip_comparison flag to the main module upon execution of the test suite:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ ./regtest.py --skip_comparison configuration_file
</pre></div>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">regression_testing</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="notification.html">Notification Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration Options</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Analysis Features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#custom-analysis-routines">Custom Analysis Routines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring-performance">Monitoring Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parameter-coverage">Parameter Coverage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#skipping-comparison">Skipping Comparison</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="configuration.html" title="previous chapter">Configuration Options</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, AMReX Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.1.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/analysis.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>